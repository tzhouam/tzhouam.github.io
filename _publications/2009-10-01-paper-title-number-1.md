---
title: "Global Prefix-Tuning: Extremely Efficient Fine-Tuning for Shallow Alignment Using One Token"
collection: publications
category: manuscripts
permalink: /publication/paper1
excerpt: 'We introduce Global-Token Tuner, an extremely parameter-efficient fine-tuning (PEFT) method for adapting Large Language Models (LLMs) that uses only a few or just one learnable token, regardless of model size. Global-Token Tuner employs a unique design that constructs a globally shared set of tunable tokens that modify the attention of every layer. Therefore no matter how base model change the tunable parameter remains relatively constant. We showed that our method can attain comparable performance with LoRA across plenty of common tasks while reducing parameter requirements from multiple millions or more to as few as 5 thousand. We also believe the discovery that even one token can effectively finetune LLMs illuminates the inner workings of LLMs.'
date: 2024-06-16
venue: 'ACL(submitted)'
paperurl: 'https://openreview.net/pdf?id=kLwY5X4ovF'

---

The contents above will be part of a list of publications, if the user clicks the link for the publication than the contents of section will be rendered as a full page, allowing you to provide more information about the paper for the reader. When publications are displayed as a single page, the contents of the above "citation" field will automatically be included below this section in a smaller font.
